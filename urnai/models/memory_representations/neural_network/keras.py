from .abneuralnetwork import ABNeuralNetwork 
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, Activation
from keras.optimizers import Adam
from keras import backend as K

class KerasDeepNeuralNetwork(ABNeuralNetwork):
    """
    Implementation of a Generic Deep Neural Network using Keras

    This class inherits from ABNeuralNetwork, so it already has all abstract methods
    necessary for learning, predicting outputs, and building a model. All that this
    class does is implement those abstract methods, and implement the methods necessary
    for adding Neural Network layers, such as add_input_layer(), add_output_layer(),
    add_fully_connected_layer(), add_convolutional_layer() etc.

    This class also implements the methodes necessary for saving and loading the model
    from local memory.

    Parameters:
        action_output_size: int
            size of our output
        state_input_shape: tuple
            shape of our input
        build_model: Python dict
            A dict representing the NN's layers. Can be generated by the 
            ModelBuilder.get_model_layout() method from an instantiated ModelBuilder object.
        gamma: Float
            Gamma parameter for the Deep Q Learning algorithm
        alpha: Float
            This is the Learning Rate of the model
        seed: Integer (default None)
            Value to assing to random number generators in Python and our ML libraries to try 
            and create reproducible experiments
        batch_size: Integer
            Size of our learning batch to be passed to the Machine Learning library
    """

    def __init__(self, action_output_size, state_input_shape, build_model, gamma, alpha, seed = None, batch_size=32):     
        super().__init__(action_output_size, state_input_shape, build_model, gamma, alpha, seed, batch_size)

        self.model.compile(optimizer=Adam(lr=self.alpha), loss='mse', metrics=['accuracy'])


    def add_input_layer(self, idx):
        self.model.add(Dense(self.build_model[idx]['nodes'], input_dim=self.build_model[idx]['shape'][1], activation='relu'))

    def add_output_layer(self, idx):
        self.model.add(Dense(self.build_model[idx]['length'], activation='linear'))

    def add_fully_connected_layer(self, idx):
        self.model.add(Dense(self.build_model[idx]['nodes'], activation='relu'))

    def add_convolutional_layer(self, idx):
        # if idx is zero it means this is an input layer, so we pass input_shape to keras
        if idx == 0:
            self.model.add(     Conv2D(
                                filters=self.build_model[idx]['filters'], 
                                kernel_size=self.build_model[idx]['filter_shape'], 
                                activation = self.build_model[idx]['activation'],
                                padding = self.build_model[idx]['padding'],
                                strides = self.build_model[idx]['strides'],
                                data_format = self.build_model[idx]['data_format'],
                                dilation_rate = self.build_model[idx]['dilation_rate'],
                                groups = self.build_model[idx]['groups'],
                                input_shape = self.build_model[idx]['input_shape']
                                )
                            )
        else:
            self.model.add(     Conv2D(
                                filters=self.build_model[idx]['filters'], 
                                kernel_size=self.build_model[idx]['filter_shape'], 
                                activation=self.build_model[idx]['activation'],
                                padding = self.build_model[idx]['padding'],
                                strides = self.build_model[idx]['strides'],
                                data_format = self.build_model[idx]['data_format'],
                                dilation_rate = self.build_model[idx]['dilation_rate'],
                                groups = self.build_model[idx]['groups']
                                )
                            )

    def add_maxpooling_layer(self, idx):
        self.model.add(     MaxPooling2D(
                            pool_size=self.build_model[idx]['pool_size'],
                            strides=self.build_model[idx]['strides'],
                            padding=self.build_model[idx]['padding'],
                            data_format=self.build_model[idx]['data_format']
                            )
                        )
    
    def add_flatten_layer(self, idx):
        self.model.add(Flatten())

    def update(self, state, target_output):
        loss = self.model.fit(state, target_output, batch_size=self.batch_size, shuffle=False, verbose=0)

    def get_output(self, state):
        return self.model.predict(state)

    def set_seed(self, seed) -> None:  
        if seed != None:
            import tensorflow as tf
            tf.random.set_seed(seed)
        return seed

    def create_base_model(self):
        model = Sequential()
        return model

    def save_extra(self, persist_path):
        self.model.save_weights(self.get_full_persistance_path(persist_path)+".h5")

    def load_extra(self, persist_path):
        import os
        exists = os.path.isfile(self.get_full_persistance_path(persist_path)+".h5")

        if(exists):
            self.__init__(self.action_output_size, self.state_input_shape, self.build_model, self.gamma, self.alpha, self.seed, self.batch_size)
            self.model.load_weights(self.get_full_persistance_path(persist_path)+".h5")
    
    def copy_model_weights(self, model_to_copy):
        self.model.set_weights(model_to_copy.model.get_weights())

class DNNCustomModelOverrideExample(KerasDeepNeuralNetwork):
    """
        This Class is a custom handcrafted Keras Neural Network

        To create a custom model we override the make_model() method from KerasDeepNeuralNetwork, 
        and then define our model architecture.

        This sort of inheritance and override is very useful for advanced users of Keras, since it lets them
        customize a model to have whichever Neural Network architecture they desire, and then be assured that
        this model will still work like any other out-of-the-box URNAI models (KerasDeepNeuralNetwork for example),
        saving itself to disk, and supporting the automatic generation of graphs.

        In order to use this class, we just need to pass it as the "neural_net_class" argument in one of our
        Reinforcement Learning algorithms, such as DeepQLearning (urnai.models.algorithms.dql) or 
        DoubleDeepQLearning (urnai.models.algorithms.ddql).

    """
    def __init__(self, action_output_size, state_input_shape, build_model, gamma, alpha, seed = None, batch_size=32):        
        super().__init__(action_output_size, state_input_shape, build_model, gamma, alpha, seed, batch_size)
    
    def make_model(self):
        self.model = Sequential()

        self.model.add(Dense(50, activation='relu', input_dim=self.state_input_shape))
        #self.model.add(Dense(50, activation='relu'))
        self.model.add(Dense(self.action_output_size, activation='linear'))

        self.model.compile(optimizer=Adam(lr=self.alpha), loss='mse', metrics=['accuracy'])