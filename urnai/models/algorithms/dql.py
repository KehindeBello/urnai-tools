import numpy
import random
from collections import deque
from models.memory_representations.neural_network.keras import KerasDeepNeuralNetwork
from models.memory_representations.neural_network.pytorch import PyTorchDeepNeuralNetwork
from models.base.abmodel import LearningModel
from agents.actions.base.abwrapper import ActionWrapper
from agents.states.abstate import StateBuilder
from utils.error import UnsuportedLibraryError
from utils import constants

class DeepQLearning(LearningModel):
    """
    Generalistic Deep Q Learning Algorithm. This algorithm can be used with Keras or Pytorch as a backend to store the Neural Network.

    Parameters:
        action_wrapper: Object
            Object responsible for describing possible actions
        state_builder: Object
            Object responsible for creating states from the game environment
        learning_rate: Float
            Rate at which the deep learning algorithm will learn (alpha on most mathematical representations)
        learning_rate_min: Float
            Minimum value that learning_rate will reach throught training
        learning_rate_decay: Float
            Inverse of the rate at which the learning rate will decay each episode (defaults to 1 so no decay)
        learning_rate_decay_ep_cutoff: Integer
            Episode at which learning rate decay will start (defaults to 0)
        gamma: Float
            Gamma parameter in the Deep Q Learning algorithm
        name: String
            Name of the algorithm implemented
        build_model: Python dict
            A dict representing the NN's layers. Can be generated by the ModelBuilder.get_model_layout() method from an instantiated ModelBuilder object.
        epsilon_start: Float
            Value that the epsilon from epsilon greedy strategy will start from (defaults to 1)
        epsilon_min: Float
            Minimum value that epsilon will reach trough training
        epsilon_decay: Float
            Inverse of the rate at which the epsilon value will decay each step (0.99 => 1% will decay each step)
        per_episode_epsilon_decay:  Bool
            Whether or not the epsilon decay will be done each episode, instead of each step
        use_memory: Bool
            If true the algorithm will keep an internal queue of state, action, reward and next_state tuple to sample from during training
        memory_maxlen: Integer
            Max lenght of the memory queue.
        batch_size: Integer
            Size of our learning batch to be passed to the Machine Learning library
        min_memory_size: Integer
            Minimum length of the memory queue in order to start training (it's customary to acumulate some thousand tuples before commencing training)
        seed_value: Integer (default None)
            Value to assing to random number generators in Python and our ML libraries to try and create reproducible experiments
        cpu_only: Bool
            If true will run algorithm only using CPU, also useful for reproducibility since GPU paralelization creates uncertainty
        lib: String
            Name of the Machine Learning library that should be used with the instanced Deep Q Learning algorithm (names of accepted libraries are defined in urnai.utils.constants.Libraries)
    """
    #by default learning rate should not decay at all, since this is not the default behavior of Deep-Q Learning

    def __init__(self, action_wrapper: ActionWrapper, state_builder: StateBuilder, learning_rate=0.001, learning_rate_min=0.0001, learning_rate_decay=1, 
                learning_rate_decay_ep_cutoff=0, gamma=0.99, name='DeepQLearning', build_model = None, epsilon_start=1.0, epsilon_min=0.005, epsilon_decay=0.99995, 
                per_episode_epsilon_decay=False, use_memory=True, memory_maxlen=50000, batch_size=32, min_memory_size=2000, seed_value=None, cpu_only=False, lib='keras'):
        super().__init__(action_wrapper, state_builder, gamma, learning_rate, learning_rate_min, learning_rate_decay, epsilon_start, epsilon_min, epsilon_decay , per_episode_epsilon_decay, learning_rate_decay_ep_cutoff, name, seed_value, cpu_only)
        # Defining the model's layers. Tensorflow's objects are stored into self.model_layers
        self.batch_size = batch_size
        self.build_model = build_model
        self.lib = lib

        if self.lib in constants.listoflibs:
            if self.lib == constants.Libraries.KERAS:
                self.dnn = KerasDeepNeuralNetwork(self.action_size, self.state_size, self.build_model, self.gamma, self.learning_rate, self.seed_value, self.batch_size)
            
            if self.lib == constants.Libraries.PYTORCH:
                self.dnn = PyTorchDeepNeuralNetwork(self.action_size, self.state_size, self.build_model, self.gamma, self.learning_rate, self.seed_value)

            # if self.lib == constants.Libraries.TENSORFLOW:
                # self.dnn = tensorflowDNN call here
        else:
            raise UnsuportedLibraryError(self.lib)

        self.use_memory = use_memory
        if self.use_memory:
            self.memory = deque(maxlen=memory_maxlen)
            self.memory_maxlen = memory_maxlen
            self.min_memory_size = min_memory_size

    def learn(self, s, a, r, s_, done):
        if self.use_memory:
            self.memory_learn(s, a, r, s_, done)
        else:
            self.no_memory_learn(s, a, r, s_, done)

    def memory_learn(self, s, a, r, s_, done):
        self.memorize(s, a, r, s_, done)
        if len(self.memory) < self.min_memory_size:
            return

        batch = random.sample(self.memory, self.batch_size)
        states = numpy.array([val[0] for val in batch])
        states = numpy.squeeze(states)
        next_states = numpy.array([(numpy.zeros(self.state_size)
                                if val[3] is None else val[3]) for val in batch])
        next_states = numpy.squeeze(next_states)

        # predict Q(s,a) given the batch of states
        # rows = self.batch_size
        # cols = self.action_size
        # q_s_a = numpy.zeros(shape=(rows, cols))
        q_s_a = self.dnn.get_output(states)
        
        #TODO: Try and implement this generically for different Libraries 
        # (tf, keras and pytorch all probably have some built-in way of batch inference)
        # for i in range(self.batch_size):
        #     q_s_a[i] = self.dnn.get_output(states[i])

        # predict Q(s',a') - so that we can do gamma * max(Q(s'a')) below
        # rows = self.batch_size
        # cols = self.action_size
        # q_s_a_d = numpy.zeros(shape=(rows, cols))
        q_s_a_d = self.dnn.get_output(next_states)
        
        #TODO: Try and implement this generically for different Libraries 
        # (tf, keras and pytorch all probably have some built-in way of batch inference)
        # for i in range(self.batch_size):
        #     q_s_a_d[i] = self.dnn.get_output(next_states[i])

        # setup training arrays
        target_q_values = numpy.zeros((len(batch), self.action_size))

        for i, (state, action, reward, next_state, done) in enumerate(batch):
            # get the current q values for all actions in state
            current_q = numpy.copy(q_s_a[i])
            if done:
                # if this is the last step, there is no future max q value, so we the new_q is just the reward
                current_q[action] = reward
            else:
                # new Q-value is equal to the reward at that step + discount factor * the max q-value for the next_state
                current_q[action] = reward + self.gamma * numpy.amax(q_s_a_d[i])
            
            target_q_values[i] = current_q

        # update neural network with expected q values
        # for i in range(self.batch_size):
        #     self.dnn.update(states[i], target_q_values[i])
        self.dnn.update(states, target_q_values)

    def no_memory_learn(self, s, a, r, s_, done):
        #get output for current sars array
        rows = 1 
        cols = self.action_size
        target_q_values = numpy.zeros(shape=(rows, cols))

        expected_q = 0
        if done:
            expected_q = r
        else:
            expected_q = r + self.gamma * self.__maxq(s_)

        target_q_values[0, a] = expected_q 

        self.dnn.update(s, target_q_values)

    def __maxq(self, state):
        values = self.dnn.get_output(state)
        mxq = values.max()
        return mxq

    def choose_action(self, state, excluded_actions=[], is_testing=False):
        if is_testing:
            return self.predict(state, excluded_actions)
        else:
            if numpy.random.rand() <= self.epsilon_greedy:
                random_action = random.choice(self.actions)

                # Removing excluded actions
                while random_action in excluded_actions:
                    random_action = random.choice(self.actions)
                return random_action
            else:
                return self.predict(state, excluded_actions)

    def predict(self, state, excluded_actions=[]):
        q_values = self.dnn.get_output(state)
        action_idx = numpy.argmax(q_values)

        # Removing excluded actions
        # TODO: This is possibly badly optimized, eventually look back into this
        while action_idx in excluded_actions:
            q_values = numpy.delete(q_values, action_idx)
            action_idx = numpy.argmax(q_values)
        
        action = int(action_idx)
        return action

    def memorize(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def save_extra(self, persist_path):
        self.dnn.save(persist_path)

    def load_extra(self, persist_path):
        self.dnn.load(persist_path)
